## ğŸ `feed_fetcher.py`
#!/usr/bin/env python3
"""RSS -> Markdown ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å®Ÿè¡Œã™ã‚‹ã¨ astro-site/src/content/posts/ ã« .md ã‚’è¿½åŠ ã™ã‚‹ã€‚
"""
import feedparser
import requests
import trafilatura
import yaml
import hashlib
import os
from datetime import datetime
from pathlib import Path
import re

ROOT = Path(__file__).resolve().parent
CONTENT_DIR = ROOT / "astro-site" / "src" / "content" / "posts"
CONTENT_DIR.mkdir(parents=True, exist_ok=True)

with open(ROOT / "rss_sources.yml", "r", encoding="utf-8") as f:
    SOURCES = yaml.safe_load(f)["sources"]

DATE_FMT_MD = "%Y-%m-%d"


def slugify(text: str) -> str:
    # å°æ–‡å­—åŒ–ãƒ»ç©ºç™½â†’ãƒã‚¤ãƒ•ãƒ³
    text = text.lower().replace(" ", "-")
    # Windowsã§ç¦æ­¢ã•ã‚Œã‚‹æ–‡å­—ã‚’é™¤å»
    text = re.sub(r'[\\/:*?"<>|]', '', text)
    # ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚„ç–‘å•ç¬¦ã€ã‚¢ãƒ³ãƒ‘ã‚µãƒ³ãƒ‰ãªã©ã‚‚æ•´å½¢
    text = text.replace("/", "-").replace("&", "and")
    return text[:80]


def fetch_article_content(url: str) -> str:
    try:
        res = requests.get(url, timeout=10)
        res.raise_for_status()
        feed = feedparser.parse(res.text)
        return trafilatura.extract(res.text) or ""
    except Exception as e:
        print(f"[ERROR] fail extract {url}: {e}")
        return ""


def make_frontmatter(meta: dict) -> str:
    import yaml as _y

    fm = _y.dump(meta, allow_unicode=True, sort_keys=False)
    return f"---\n{fm}---\n\n"


def main():
    print(f"[DEBUG] èª­ã¿è¾¼ã‚“ã ã‚½ãƒ¼ã‚¹æ•°: {len(SOURCES)}")
    for s in SOURCES:
        print(f"[DEBUG] ã‚½ãƒ¼ã‚¹å: {s['name']} - {s['url']}")
        # âœ… feed ã‚’ã¾ãšå–å¾—
        feed = feedparser.parse(s["url"])
        # âœ… ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯ã¨ãƒ­ã‚°å‡ºåŠ›
        if feed.bozo:
            print(f"[ERROR] {s['name']} RSSå–å¾—ã‚¨ãƒ©ãƒ¼: {feed.bozo_exception}")
        else:
            print(f"[DEBUG] {s['name']} - entriesæ•°: {len(feed.entries)}")
        # âœ… è¨˜äº‹ã®ãƒ«ãƒ¼ãƒ—ã¯
        for entry in feed.entries:
            feed = feedparser.parse(s["url"])
            print(f"[DEBUG] {s['name']} - entriesæ•°: {len(feed.entries)}")
            # ä¸€æ„ slug ã¯ (æ—¥ä»˜-title ãƒãƒƒã‚·ãƒ¥)
            date_prefix = datetime.utcnow().strftime(DATE_FMT_MD)
            slug_base = slugify(entry.title)
            uid_hash = hashlib.md5(entry.link.encode()).hexdigest()[:6]
            slug = f"{date_prefix}-{slug_base}-{uid_hash}"
            md_path = CONTENT_DIR / f"{slug}.md"
            if md_path.exists():
                continue  # æ—¢ã«å–å¾—æ¸ˆã¿

            body = fetch_article_content(entry.link)
            if not body:
                continue

            front = {
                "title": entry.title,
                "description": entry.get("summary", ""),
                "pubDate": entry.get("published", datetime.utcnow().isoformat()),
                "source": s["name"],
                "tags": s.get("tags", []),
                "url": entry.link,
            }
            with open(md_path, "w", encoding="utf-8") as f:
                f.write(make_frontmatter(front))
                f.write(body)
            print(f"[INFO] saved {md_path}")


if __name__ == "__main__":
    main()