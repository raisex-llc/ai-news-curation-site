---
title: "How AI training scales"
description: "We’ve discovered that the gradient noise scale, a simple statistical metric, predicts the parallelizability of neural network training on a wide range of tasks. Since complex tasks tend to have noisier gradients, increasingly large batch sizes are likely to become useful in the future, removing one potential limit to further growth of AI systems. More broadly, these results show that neural network training need not be considered a mysterious art, but can be rigorized and systematized."
summary: "We’ve discovered that the gradient noise scale, a simple statistical metric, predicts the parallelizability of neural network training on a wide range of tasks. Since complex tasks tend to have noisier gradients, increasingly large batch sizes are likely to become useful in the future, removing one potential limit to further growth of AI systems. More broadly, these results show that neural network training need not be considered a mysterious art, but can be rigorized and systematized."
pubDate: "Fri, 14 Dec 2018 08:00:00 GMT"
source: "OpenAI Blog"
url: "https://openai.com/blog/how-ai-training-scales"
thumbnail: "https://raisex-llc.github.io/ai-news-curation-site/assets/openai_logo.png"
---

