---
title: "LeRobot Community Datasets: The â€œImageNetâ€ of Robotics â€” When and How?"
description: ""
summary: ""
pubDate: "Sun, 11 May 2025 00:00:00 GMT"
source: "Hugging Face Blog"
url: "https://huggingface.co/blog/lerobot-datasets"
thumbnail: ""
---

LeRobot Community Datasets: The â€œImageNetâ€ of Robotics â€” When and How?
ğŸ§­ TL;DR â€” Why This Blogpost?
In this post, we:
- Recognize the growing impact of community-contributed LeRobot datasets
- Highlight the current challenges in robotic data collection and curation
- Share practical steps and best practices to maximize the impact of this collective effort
Our goal is to frame generalization as a data problem, and to show how building an open, diverse â€œImageNet of roboticsâ€ is not just possibleâ€”but already happening.
Introduction
Recent advances in Vision-Language-Action (VLA) models have enabled robots to perform a wide range of tasksâ€”from simple commands like â€œgrasp the cubeâ€ to more complex activities like folding laundry or cleaning a table. These models aim to achieve generalization: the ability to perform tasks in novel settings, with unseen objects, and in varying conditions.
â€œThe biggest challenge in robotics isnâ€™t dexterity, but generalizationâ€”across physical, visual, and semantic levels.â€
â€” Physical Intelligence
A robot must "figure out how to correctly perform even a simple task in a new setting or with new objects," and this requires both robust skills and common-sense understanding of the world. Yet, progress is often limited by the availability of diverse data for such robotic systems.
â€œGeneralization must occur at many levels. At the low level, the robot must understand how to pick up a spoon (by the handle) or plate (by the edge), even if it has not seen these specific spoons or plates before, and even if they are placed in a pile of dirty dishes. At a higher level, the robot must understand the semantics of each taskâ€”where to put clothes and shoes (ideally in the laundry hamper or closet, not on the bed), and what kind of tool is appropriate for wiping down a spill. This generalization requires both robust physical skills and a common-sense understanding of the environment, so that the robot can generalize at many levels at the same time, from physical, to visual, to semantic. This is made even harder by the limited availability of diverse data for such robotic systems.â€
â€” Physical Intelligence
From Models to Data: Shifting the Perspective
To simplify, the core of generalist policies lies in a simple idea: co-training on heterogeneous datasets. By exposing VLA models to a variety of environments, tasks, and robot embodiments, we can teach models not only how to act, but whyâ€”how to interpret a scene, understand a goal, and adapt skills across contexts.
ğŸ’¡ â€œGeneralization is not just a model propertyâ€”itâ€™s a data phenomenon.â€
It emerges from the diversity, quality, and abstraction level of the training data.
This brings us to a fundamental question:
Given current datasets, what is the upper limit of generalization we can expect?
Can a robot meaningfully respond to a completely novel promptâ€”say, *"set up a surprise birthday party"*â€”if it has never encountered anything remotely similar during training? Especially when most datasets are collected in academic labs, by a limited number of people, under well-controlled setups?
We frame generalization as a data-centric view: treating it as the process of abstracting broader patterns from dataâ€”essentially â€œzooming outâ€ to reveal task-agnostic structures and principles. This shift in perspective emphasizes the role of dataset diversity, rather than model architecture alone, in driving generalization.
Why does Robotics lack its ImageNet Moment?
So far, the majority of robotics datasets come from structured academic environments. Even if we scale up to millions of demonstrations, one dataset will often dominate, limiting diversity. Unlike ImageNetâ€”which aggregated internet-scale data and captured the real world more holisticallyâ€”robotics lacks a comparably diverse, community-driven benchmark.
This is largely because collecting data for robotics requires physical hardware and significant effort.
Building a LeRobot Community
Thatâ€™s why, at LeRobot, weâ€™re working to make robotics data collection more accessibleâ€”at home, at school, or anywhere. We're:
- Simplifying the recording pipeline
- Streamlining uploading to the Hugging Face Hub, to foster community sharing
- Reducing hardware costs
We're already seeing the results: the number of community-contributed datasets on the Hub is growing rapidly.
Growth of lerobot datasets on the Hugging Face Hub over time.
If we break down the uploaded datasets by robot type, we see that most contributions are to So100 and Koch, making robotic arms and manipulation tasks the primary focus of the current LeRobot dataset landscape. However, itâ€™s important to remember that the potential reaches far beyond. Domains like autonomous vehicles, assistive robots, and mobile navigation stand to benefit just as much from shared data. This momentum brings us closer to a future where datasets reflect a global effort, not just the contributions of a single lab or institution.
Distribution of lerobot datasets by robot type.
Here are just a few standout community-contributed datasets that show how diverse and imaginative robotics can be:
lirislab/close_top_drawer_teabox
:: precise manipulation with a household drawerChojins/chess_game_001_blue_stereo
: a full chess match captured from a stereo camera setuppierfabre/chicken
: yes â€” a robot interacting with colorful animal figures, including a chicken ğŸ”
Explore additional creative datasets under the LeRobot
tag on the Hugging Face Hub, and interactively view them in the LeRobot Dataset Visualizer.
Scaling Responsibly
As robotics data collection becomes more democratized, curation becomes the next challenge. While these datasets are still collected in constrained setups, they are a crucial step toward affordable, general-purpose robotic policies. Not everyone has access to expensive hardwareâ€”but with shared infrastructure and open collaboration, we can build something far greater.
ğŸ§  â€œGeneralization isnâ€™t solved in a labâ€”itâ€™s taught by the world.â€
The more diverse our data, the more capable our models will be.
Better data = Better models
Why does data quality matter? Poor-quality data results in poor downstream performance, biased outputs, and models that fail to generalize. Hence, efficient and high-quality data collection plays a critical role in advancing generalist robotic policies.
While foundation models in vision and language have thrived on massive, web-scale datasets, robotics lacks an â€œInternet of robotsâ€â€”a vast, diverse corpus of real-world interactions. Instead, robotic data is fragmented across different embodiments, sensor setups, and control modes, forming isolated data islands.
To overcome this, recent approaches like Gr00t organize training data as a pyramid, where:
- Large-scale web and video data form the foundation
- Synthetic data adds simulated diversity
- Real-world robot interactions at the top ground the model in physical execution
Within this framework, efficient real-world data collection is indispensableâ€”it anchors learned behaviors in actual robotic hardware and closes the sim-to-real gap, ultimately improving the generalization, adaptability, and performance of robotics foundation models.
By expanding the volume and diversity of real-world datasets, we reduce fragmentation between heterogeneous data sources. When datasets are disjoint in terms of environment, embodiment, or task distribution, models struggle to transfer knowledge across domains.
ğŸ”— Real-world data acts as connective tissueâ€”it aligns abstract priors with grounded action and enables the model to build more coherent and transferable representations.
As a result, increasing the proportion of real robot interactions does not merely enhance realismâ€”it structurally reinforces the links between all layers of the pyramid, leading to more robust and capable policies.
Data Pyramid for Robot Foundation Model Training. Adapted from Gr00t (Yang et al., 2025). Data quantity decreases while embodiment specificity increases from bottom to top.
Challenges with Current Community Datasets
At LeRobot, weâ€™ve started developing an automatic curation pipeline to post-process community datasets. During the post-processing phase, weâ€™ve identified several areas where improvements can further boost dataset quality and facilitate more effective curation going forward:
1. Incomplete or Inconsistent Task Annotations
Many datasets lack task descriptions, lack details or are ambiguous in the task to be done. Semantics is currently at the core of cognition, meaning that understanding the context and specifics of a task is crucial for robotic performance. Detailed expressions ensure that robots understand exactly what is expected, but also provide a broader knowledge and vocabulary to the cognition system. Ambiguity can lead to incorrect interpretation and, consequently, incorrect actions.
Task instructions can be:
- Empty
- Too short (e.g. â€œHoldâ€, â€œUpâ€)
- Without any specific meaning (e.g. â€œtask descâ€, â€œdescâ€)
Subtask-level annotations are often missing, making it difficult to model complex task hierarchies.
While this can be handled with VLM, it is still better to have a task annotation provided by the author of the dataset at hand.
2. Feature Mapping Inconsistencies
Features like images.laptop
are ambiguously labeled:
- Sometimes it's a third-person view
- Other times it's more like a gripper (wrist) camera
Manual mapping of dataset features to standardized names is time-consuming and error-prone.
We can possibly automate feature type inference using VLMs or computer vision models to classify camera perspectives. However, keeping this in mind helps to have a cleaner dataset.
3. Low-Quality or Incomplete Episodes
Some datasets contain:
- Episodes with only 1 or very few frames
- Manually deleted data files (e.g., deleted
.parquet
files without reindexing), breaking the sequential consistency.
4. Inconsistent Action/State Dimensions
Different datasets use different action or state dimensions, even for the same robot (e.g., so100
).
Some datasets show inconsistencies in action/state format.
What Makes a Good Dataset?
Now that we know that creating a high-quality dataset is essential for training reliable and generalizable robot policies, we have outlined a checklist of best practices to assist you in collecting effective data.
Image Quality
- âœ… Use preferably two camera views
- âœ… Ensure steady video capture (no shaking)
- âœ… Maintain neutral, stable lighting (avoid overly yellow or blue tones)
- âœ… Ensure consistent exposure and sharp focus
- âœ… Leader arm should not appear in the frame
- âœ… The only moving objects should be the follower arm and the manipulated items (avoid human limbs/bodies)
- âœ… Use a static, non-distracting background, or apply controlled variations
- âœ… Record in high resolution (at least 480x640 / 720p)
Metadata & Recording Protocol
- âœ… Select the correct robot type in the metadata
If you're using a custom robot that's not listed in the official LeRobot config registry,
we recommend checking how similar robots are named in existing datasets on the LeRobot Hub to ensure consistency. - âœ… Record videos at approximately 30 frames per second (FPS)
- âœ… If deleting episodes, make sure to update the metadata files accordingly (we will provide proper tools to edit datasets)
Feature Naming Conventions
Use a consistent and interpretable naming scheme for all camera views and observations:
Format:
<modality>.<location>
Examples:
images.top
images.front
images.left
images.right
Avoid device-specific names:
- âŒ
images.laptop
- âŒ
images.phone
For wrist-mounted cameras, specify orientation:
images.wrist.left
images.wrist.right
images.wrist.top
images.wrist.bottom
Consistent naming improves clarity and helps downstream models better interpret spatial configurations and multi-view inputs.
Task Annotation
- âœ… Use the
task
field to clearly describe the robotâ€™s objective- Example:
Pick the yellow lego block and put it in the box
- Example:
- âœ… Keep task descriptions concise (between 25â€“50 characters)
- âœ… Avoid vague or generic names like
task1
,demo2
, etc.
Below, we provide a checklist that serves as a guideline for recording datasets, outlining key points to keep in mind during the data collection process.
Figure 4: Dataset Recording Checklist â€“ a step-by-step guide to ensure consistent and high-quality real-world data collection.
How Can You Help?
The next generation of generalist robots won't be built by a single person or lab â€” they'll be built by all of us. Whether you're a student, a researcher, or just robot-curious, hereâ€™s how you can jump in:
- ğŸ¥ Record your own datasets â€” Use LeRobot tools to capture and upload good quality datasets from your robots.
- ğŸ§  Improve dataset quality â€” Follow our checklist, clean up your recordings, and help set new standards for robotics data.
- ğŸ“¦ Contribute to the Hub â€” Upload datasets, share examples, and explore what others are building.
- ğŸ’¬ Join the conversation â€” Give feedback, request features, or help shape the roadmap by engaging in our LeRobot Discord Server.
- ğŸŒ Grow the movement â€” Introduce LeRobot to your club, classroom, or lab. More contributors = better generalization.
Start recording, start contributingâ€”because the future of generalist robots depends on the data we build today.