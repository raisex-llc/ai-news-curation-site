---
title: "The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions"
description: "Today's LLMs are susceptible to prompt injections, jailbreaks, and other attacks that allow adversaries to overwrite a model's original instructions with their own malicious prompts."
summary: "Today's LLMs are susceptible to prompt injections, jailbreaks, and other attacks that allow adversaries to overwrite a model's original instructions with their own malicious prompts."
pubDate: "Fri, 19 Apr 2024 19:00:00 GMT"
source: "OpenAI Blog"
url: "https://openai.com/blog/the-instruction-hierarchy"
thumbnail: "https://raisex-llc.github.io/ai-news-curation-site/assets/openai_logo.png"
---

